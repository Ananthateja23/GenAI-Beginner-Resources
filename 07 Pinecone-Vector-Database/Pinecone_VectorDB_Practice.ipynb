{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgneUr00sRVb"
      },
      "outputs": [],
      "source": [
        "!pip install langchain-community"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pypdf"
      ],
      "metadata": {
        "id": "rOwdqCH1B6Eq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain-community pypdf"
      ],
      "metadata": {
        "id": "R0Pls6m4-8Bp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Load the PDF Files\n",
        "## Extract the Text from the PDF's"
      ],
      "metadata": {
        "id": "sveeQTCGsU1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir pdfs"
      ],
      "metadata": {
        "id": "W_BAQZkGxKr7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "# loader = PyPDFDirectoryLoader(\"/content/pdfs\")\n",
        "# documents = loader.load()"
      ],
      "metadata": {
        "id": "eKiiEdaM_Txs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_community.document_loaders import DirectoryLoader, PyPDFLoader\n",
        "\n",
        "# 1. Define the path to your folder\n",
        "path = \"/content/pdfs\"\n",
        "\n",
        "# 2. Setup the DirectoryLoader\n",
        "# glob=\"./*.pdf\" ensures we only grab PDF files\n",
        "# loader_cls=PyPDFLoader tells LangChain to use the PDF parser for each file found\n",
        "loader = DirectoryLoader(\n",
        "    path,\n",
        "    glob=\"./*.pdf\",\n",
        "    loader_cls=PyPDFLoader\n",
        ")\n",
        "\n",
        "# 3. Load the documents\n",
        "documents = loader.load()\n",
        "\n",
        "documents"
      ],
      "metadata": {
        "id": "B4w5kTeOBj7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split the Extracted Data into Text Chunks"
      ],
      "metadata": {
        "id": "8gxzD90Ispr5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "# text_chunks = text_splitter.split_documents(documents)\n",
        "# text_chunks"
      ],
      "metadata": {
        "id": "rNzxf6d___dO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# Split documents\n",
        "text_splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=20)\n",
        "text_chunks = text_splitter.split_documents(documents)\n",
        "text_chunks"
      ],
      "metadata": {
        "id": "T0GCP6pEw4_w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text_chunks)"
      ],
      "metadata": {
        "id": "lk6vU2b6xqlB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks[1]"
      ],
      "metadata": {
        "id": "Lucx0OlGC6Il"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks[2]"
      ],
      "metadata": {
        "id": "_VXiz5b4C-39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_chunks[3]"
      ],
      "metadata": {
        "id": "D_BLSX0lDAd-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downlaod the Embeddings"
      ],
      "metadata": {
        "id": "ExSL4PPtswQU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain langchain-openai"
      ],
      "metadata": {
        "id": "cRsd8LdgDNxT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_openai import OpenAIEmbeddings\n",
        "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "# result = embeddings.embed_query(\"How are you?\")\n",
        "# print(result)\n",
        "# print(len(result))"
      ],
      "metadata": {
        "id": "mJIWbgeDAz1F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "\n",
        "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "Mjbhw2PkEjdV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY"
      ],
      "metadata": {
        "id": "-zt6K5nGEzZ-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "\n",
        "result = embeddings.embed_query(\"How are you!\")\n",
        "\n",
        "len(result)"
      ],
      "metadata": {
        "id": "oviONm7dE5Rc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "id": "a8hwkzSfH1zs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U pinecone-client langchain-pinecone"
      ],
      "metadata": {
        "id": "48jDkC1vRxDW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Initializing the Pinecone"
      ],
      "metadata": {
        "id": "RI0_TL4Gs6W-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
        "# loader = PyPDFDirectoryLoader(\"/content/pdfs\")\n",
        "# documents = loader.load()\n",
        "\n",
        "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
        "# text_chunks = text_splitter.split_documents(documents)\n",
        "# text_chunks\n",
        "\n",
        "# import os\n",
        "# os.environ[\"OPENAI_API_KEY\"] = OPENAI_API_KEY\n",
        "\n",
        "# from langchain_openai import OpenAIEmbeddings\n",
        "# embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "# result = embeddings.embed_query(\"How are you?\")\n",
        "# print(result)\n",
        "# print(len(result))\n",
        "\n",
        "# PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "\n",
        "# from pinecone import Pinecone\n",
        "# pc = Pinecone(api_key = PINECONE_API_KEY)\n",
        "\n",
        "\n",
        "# os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "# from langchain_pinecone import PineconeVectorStore\n",
        "# index_name = \"my-first-pineconedb-project\"\n",
        "# docsearch = PineconeVectorStore.from_texts(\n",
        "#     embedding = embeddings,\n",
        "#     index_name = index_name,\n",
        "#     texts = [t.page_content for t in text_chunks]\n",
        "# )\n",
        "# print(docsearch)\n",
        "\n",
        "# docsearch = PineconeVectorStore.from_existing_index(\n",
        "#     embedding = embeddings,\n",
        "#     index_name = index_name\n",
        "# )\n",
        "# print(docsearch)\n",
        "\n",
        "# query = \"How are you?\"\n",
        "# docs = docsearch.similarity_score(query, k=3)\n",
        "# print(docs)\n",
        "\n",
        "\n",
        "\n",
        "# from langchain_openai import ChatOpenAI\n",
        "# from langchain_core.prompts import ChatPromptTemplate\n",
        "# from langchain_core.runnables import RunnablePassthrough\n",
        "# from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# llm = ChatOpenAI(\n",
        "#     model = \"gpt-4o-mini\",\n",
        "#     temperature = 0,\n",
        "#     max_tokens = 50\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "# prompt = ChatPromptTemplate.from_template(\"\"\" You are a helpful assistant. Use ONLY the context below to answer the question.\n",
        "# If the answer is not in the context, say \"I don't know\".\n",
        "# Context: {context}\n",
        "# Question: {question}\n",
        "# Answer: \"\"\")\n",
        "\n",
        "\n",
        "# rag_chain = (\n",
        "#     {\n",
        "#         \"context\": docsearch.as_retriever(),\n",
        "#         \"question\": RunnablePassthrough()\n",
        "#     }\n",
        "#     | prompt\n",
        "#     | llm\n",
        "#     | StrOutputParser()\n",
        "# )\n",
        "\n",
        "# rag_chain.invoke(\"YOLOv7 outperforms which models\")"
      ],
      "metadata": {
        "id": "-1ragsWoBhrF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# PINECONE_API_KEY = os.getenv(\"PINECONE_API_KEY\")\n",
        "\n",
        "# from pinecone import Pinecone\n",
        "# pc = Pinecone(api_key = PINECONE_API_KEY)\n",
        "\n",
        "\n",
        "# os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY\n",
        "\n",
        "# from langchain_pinecone import PineconeVectorStore\n",
        "# index_name = \"my-first-pineconedb-project\"\n",
        "# docsearch = PineconeVectorStore.from_texts(\n",
        "#     embedding = embeddings,\n",
        "#     index_name = index_name,\n",
        "#     texts = [t.page_content for t in text_chunks]\n",
        "# )\n",
        "# print(docsearch)\n",
        "\n",
        "# docsearch = PineconeVectorStore.from_existing_index(\n",
        "#     embedding = embeddings,\n",
        "#     index_name = index_name\n",
        "# )\n",
        "# print(docsearch)\n",
        "\n",
        "# query = \"How are you?\"\n",
        "# docs = docsearch.similarity_score(query, k=3)\n",
        "# print(docs)"
      ],
      "metadata": {
        "id": "BBAppIWCB0jg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "PINECONE_API_KEY = userdata.get('PINECONE_API_KEY')"
      ],
      "metadata": {
        "id": "VSt0TudXSU9Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "os.environ[\"PINECONE_API_KEY\"] = PINECONE_API_KEY"
      ],
      "metadata": {
        "id": "CyQuRNDjXdBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from pinecone import Pinecone\n",
        "from langchain_pinecone import PineconeVectorStore\n",
        "\n",
        "# 1. Initialize the Pinecone client\n",
        "pc = Pinecone(api_key=PINECONE_API_KEY)"
      ],
      "metadata": {
        "id": "duBhSc3STKlP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pc.has_index(\"my-first-pineconedb-project\")"
      ],
      "metadata": {
        "id": "gX6wfc6bUzTZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create Embeddings for each of the Text Chunk"
      ],
      "metadata": {
        "id": "uVXPjVrBs_PT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. To Create/Upload new documents (Replacement for .from_texts)\n",
        "# Note: 'embeddings' should be your embedding model instance (e.g., OpenAIEmbeddings)\n",
        "index_name = \"my-first-pineconedb-project\"\n",
        "docsearch = PineconeVectorStore.from_texts(\n",
        "    texts=[t.page_content for t in text_chunks],\n",
        "    embedding=embeddings,\n",
        "    index_name=index_name\n",
        ")\n",
        "\n",
        "docsearch"
      ],
      "metadata": {
        "id": "AwdyUw7YMb3_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## If you already have an index, you can load it like this"
      ],
      "metadata": {
        "id": "5XYElFJ_tD0o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. To Load an existing index (Replacement for .from_existing_index)\n",
        "docsearch = PineconeVectorStore.from_existing_index(\n",
        "    index_name=index_name,\n",
        "    embedding=embeddings\n",
        ")\n",
        "\n",
        "docsearch"
      ],
      "metadata": {
        "id": "7zMuzMEVVSa3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Similarity Search"
      ],
      "metadata": {
        "id": "0kc3qU-ltMUH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# docsearch.as_retriever()"
      ],
      "metadata": {
        "id": "VXQmamKRIKKE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Perform Similarity Search\n",
        "query = \"YOLOv7 outperforms which models\"\n",
        "docs = docsearch.similarity_search(query, k=3)\n",
        "\n",
        "print(docs)"
      ],
      "metadata": {
        "id": "sOIaFBLxXxKo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "docs"
      ],
      "metadata": {
        "id": "SCL8nATmX4SJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install langchain"
      ],
      "metadata": {
        "id": "oL43_PN7ai3A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating a LLM Model Wrapper"
      ],
      "metadata": {
        "id": "iSkcp3U9tZMZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_openai import ChatOpenAI\n",
        "# from langchain_core.prompts import ChatPromptTemplate\n",
        "# from langchain_core.runnables import RunnablePassthrough\n",
        "# from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "# llm = ChatOpenAI(\n",
        "#     model = \"gpt-4o-mini\",\n",
        "#     temperature = 0,\n",
        "#     max_tokens = 50\n",
        "# )\n",
        "\n",
        "\n",
        "\n",
        "# prompt = ChatPromptTemplate.from_template(\"\"\" You are a helpful assistant. Use ONLY the context below to answer the question.\n",
        "# If the answer is not in the context, say \"I don't know\".\n",
        "# Context: {context}\n",
        "# Question: {question}\n",
        "# Answer: \"\"\")\n",
        "\n",
        "\n",
        "# rag_chain = (\n",
        "#     {\n",
        "#         \"context\": docsearch.as_retriever(),\n",
        "#         \"question\": RunnablePassthrough()\n",
        "#     }\n",
        "#     | prompt\n",
        "#     | llm\n",
        "#     | StrOutputParser()\n",
        "# )\n",
        "\n",
        "# rag_chain.invoke(\"YOLOv7 outperforms which models\")"
      ],
      "metadata": {
        "id": "7prFjZMrGYbB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_openai import ChatOpenAI\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from operator import itemgetter"
      ],
      "metadata": {
        "id": "2QMd1mv4cP0s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1️) LLM\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\")"
      ],
      "metadata": {
        "id": "F_FlWTBKcQkZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 2️) Prompt template\n",
        "prompt = ChatPromptTemplate.from_template(\n",
        "    \"\"\"\n",
        "    Answer the question using ONLY the context below.\n",
        "    If the answer is not in the context, say \"I don't know\".\n",
        "\n",
        "    Context:\n",
        "    {context}\n",
        "\n",
        "    Question:\n",
        "    {question}\n",
        "    \"\"\"\n",
        ")\n",
        "\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "H7Ig7nz-chv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3️) LCEL RAG Chain\n",
        "\n",
        "full_rag_chain = (\n",
        "    RunnablePassthrough.assign(context=itemgetter(\"question\") | docsearch.as_retriever())\n",
        "    | {\n",
        "        \"answer\": prompt | llm | StrOutputParser(),\n",
        "        \"context\": itemgetter(\"context\")\n",
        "    }\n",
        ")\n",
        "\n",
        "# full_rag_chain"
      ],
      "metadata": {
        "id": "JlsUfIBAc3en"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Helper function\n",
        "def process_llm_response(response):\n",
        "    print(\"Answer:\\n\")\n",
        "    print(response[\"answer\"])\n",
        "    print(\"\\nSources:\")\n",
        "    for doc in response[\"context\"]:\n",
        "        source = doc.metadata.get(\"source\", \"Unknown source\")\n",
        "        print(f\"- {source}\")"
      ],
      "metadata": {
        "id": "M9Vs9EOJdrmu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q/A"
      ],
      "metadata": {
        "id": "BwLotb8ctcyi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ' YOLOv7 outperforms YOLOv5-L6 (r6.1), YOLOX-X, YOLOR-E6, PPYOLOE-X, YOLOv7-D6, YOLOv5-X6 (r6.1), YOLOv7-E6E, YOLOv5-X (r6.1), YOLOR-CSP, YOLOR-CSP-X, YOLOv7-tiny-SiLU, YOLOv7, and YOLOv7-X.'"
      ],
      "metadata": {
        "id": "VzghKXqIeZzc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full example\n",
        "query = \"YOLOv7 outperforms which models\"\n",
        "response = full_rag_chain.invoke({\"question\": query})\n",
        "process_llm_response(response)"
      ],
      "metadata": {
        "id": "RIq7dSyLdz9s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        " ' Rachel Green has a PhD in English from the University of Illinois at Urbana-Champaign. Her dissertation title was \"Down on the Farm: World War One and the Emergence of Literary Modernism in the American South\". She also holds an MA in English from Butler University, and has received a Summer Research Grant from the Center for Summer Studies, a Graduate College Conference Travel Grant from the University of Illinois, the Most Outstanding Butler Woman award from Butler University, and an Academic Scholarship from Butler University. She has published multiple works, and has presented at conferences.'"
      ],
      "metadata": {
        "id": "svqDK9anegT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Full example\n",
        "query = \"Rachel Green Experience\"\n",
        "response = full_rag_chain.invoke({\"question\": query})\n",
        "process_llm_response(response)"
      ],
      "metadata": {
        "id": "wb5c6lm_d3bL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from langchain_core.prompts import ChatPromptTemplate\n",
        "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "# from langchain.chains import create_retrieval_chain\n",
        "# from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
        "# from langchain_community.vectorstores import FAISS\n",
        "\n",
        "# # ... (define your vector store and retriever, llm) ...\n",
        "\n",
        "# # Example boilerplate to set up components\n",
        "# llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)\n",
        "# embeddings = OpenAIEmbeddings()\n",
        "# docs = [\"Buildings are made out of brick\", \"Cars are made out of metal\"]\n",
        "# vectorstore = FAISS.from_texts(docs, embeddings)\n",
        "# retriever = vectorstore.as_retriever()\n",
        "\n",
        "# # Define a prompt template\n",
        "# prompt = ChatPromptTemplate.from_template(\"\"\"Answer the user's question: {input} based on the following context {context}\"\"\")\n",
        "\n",
        "# # Create the chains\n",
        "# combine_docs_chain = create_stuff_documents_chain(llm, prompt)\n",
        "# retrieval_chain = create_retrieval_chain(retriever, combine_docs_chain)\n",
        "\n",
        "# # Invoke the chain\n",
        "# response = retrieval_chain.invoke({\"input\": \"What are cars made of?\"})\n",
        "# print(response[\"answer\"])\n"
      ],
      "metadata": {
        "id": "kUQkHdDn4HSe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}